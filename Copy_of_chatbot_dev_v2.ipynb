{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Copy of chatbot_dev_v2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jackxie1011/chatterbot_django/blob/master/Copy_of_chatbot_dev_v2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SFLmVQaDR2VS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "import numpy as np\n",
        "import zipfile\n",
        "from matplotlib import pyplot as plt\n",
        "%matplotlib inline\n",
        "import sys\n",
        "import datetime\n",
        "\n",
        "#downloading weights and cofiguration file for the model\n",
        "#!wget https://storage.googleapis.com/bert_models/2018_10_18/uncased_L-12_H-768_A-12.zip\n",
        "#repo = 'model_repo'\n",
        "#with zipfile.ZipFile(\"uncased_L-12_H-768_A-12.zip\",\"r\") as zip_ref:\n",
        "#    zip_ref.extractall(repo)\n",
        "\n",
        "#!ls 'model_repo/uncased_L-12_H-768_A-12'\n",
        "\n",
        "#!ls 'model_repo'\n",
        "\n",
        "!wget https://raw.githubusercontent.com/google-research/bert/master/modeling.py \n",
        "!wget https://raw.githubusercontent.com/google-research/bert/master/optimization.py \n",
        "!wget https://raw.githubusercontent.com/google-research/bert/master/run_squad.py \n",
        "!wget https://raw.githubusercontent.com/google-research/bert/master/tokenization.py \n",
        "\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnkjXHFBsEPT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Available pretrained model checkpoints:\n",
        "#   uncased_L-12_H-768_A-12: uncased BERT base model\n",
        "#   uncased_L-24_H-1024_A-16: uncased BERT large model\n",
        "#   cased_L-12_H-768_A-12: cased BERT large model\n",
        "#We will use the most basic of all of them\n",
        "\n",
        "#BERT_MODEL = 'uncased_L-12_H-768_A-12'\n",
        "#BERT_PRETRAINED_DIR = f'{repo}/uncased_L-12_H-768_A-12'\n",
        "#OUTPUT_DIR = f'{repo}/outputs'\n",
        "#print(f'***** Model output directory: {OUTPUT_DIR} *****')\n",
        "#print(f'***** BERT pretrained directory: {BERT_PRETRAINED_DIR} *****')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oYtAmrh9JERP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pprint\n",
        "import json\n",
        "import os\n",
        "\n",
        "assert 'COLAB_TPU_ADDR' in os.environ, 'ERROR: Not connected to a TPU runtime; please see the first cell in this notebook for instructions!'\n",
        "TPU_ADDRESS = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "print('TPU address is', TPU_ADDRESS)\n",
        "\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "#disable\n",
        "with tf.compat.v1.Session(TPU_ADDRESS) as session:\n",
        "  print('TPU devices:')\n",
        "  pprint.pprint(session.list_devices())\n",
        "\n",
        "  # Upload credentials to TPU.\n",
        "  with open('/content/adc.json', 'r') as f:\n",
        "    auth_info = json.load(f)\n",
        "  #tf.contrib.cloud.configure_gcs(session, credentials=auth_info)\n",
        "  # Now credentials are set for all future sessions on this TPU.\n",
        "\n",
        "BERT_MODEL = 'uncased_L-12_H-768_A-12'\n",
        "BERT_PRETRAINED_DIR = 'gs://cloud-tpu-checkpoints/bert/' + BERT_MODEL \n",
        "print('***** BERT pretrained directory: {} *****'.format(BERT_PRETRAINED_DIR))\n",
        "!gsutil ls $BERT_PRETRAINED_DIR\n",
        "\n",
        "TASK = 'chatbot'\n",
        "BUCKET = 'chatbot-1'\n",
        "assert BUCKET, 'Must specify an existing GCS bucket name'\n",
        "OUTPUT_DIR = 'gs://{}/bert-tfhub/models/{}'.format(BUCKET, TASK)\n",
        "tf.io.gfile.makedirs(OUTPUT_DIR)\n",
        "print('***** Model output directory: {} *****'.format(OUTPUT_DIR))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4iLBzEntCKqI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!test -d bert_repo || git clone https://github.com/google-research/bert bert_repo\n",
        "if not 'bert_repo' in sys.path:\n",
        "  sys.path += ['bert_repo']\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FlNjHLWys2jh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def is_whitespace(c):\n",
        "    if c == \" \" or c == \"\\t\" or c == \"\\r\" or c == \"\\n\" or ord(c) == 0x202F:\n",
        "        return True\n",
        "    return False\n",
        "def get_squad_attributes(row):\n",
        "    orig_answer_text = row['Answer']\n",
        "    doc_tokens = []\n",
        "    prev_is_whitespace = True\n",
        "    is_impossible = False\n",
        "    start_position = 0\n",
        "    end_position = 0\n",
        "\n",
        "    if type(orig_answer_text) is float:\n",
        "        orig_answer_text = str(orig_answer_text)\n",
        "        is_impossible = True\n",
        "    else:\n",
        "        if len(orig_answer_text) == 0:\n",
        "            is_impossible = True\n",
        "    if is_impossible == False:\n",
        "        for c in orig_answer_text:\n",
        "            if is_whitespace(c):\n",
        "                prev_is_whitespace = True\n",
        "            else:\n",
        "                if prev_is_whitespace:\n",
        "                    doc_tokens.append(c)\n",
        "                else:\n",
        "                    doc_tokens[-1] += c\n",
        "                prev_is_whitespace = False\n",
        "        \n",
        "        answer_length = len(orig_answer_text)\n",
        "        start_position = 0\n",
        "        end_position = answer_length - 1\n",
        "\n",
        "    if len(doc_tokens) <= 0:\n",
        "        is_impossible = True\n",
        "    else:\n",
        "        actual_text = \" \".join(\n",
        "            doc_tokens[start_position:(end_position + 1)])\n",
        "        cleaned_answer_text = \" \".join(\n",
        "            tokenization.whitespace_tokenize(orig_answer_text))\n",
        "\n",
        "        if actual_text.find(cleaned_answer_text) == -1:\n",
        "            tf.logging.warning(\"Could not find answer: '%s' vs. '%s'\",\n",
        "                                 actual_text, cleaned_answer_text)\n",
        "  \n",
        "    return pd.Series({'doc_tokens':doc_tokens, \n",
        "                      'origin_question':str(row['Question']),\n",
        "                      'orig_answer_text':orig_answer_text,\n",
        "                      'start_position': start_position,\n",
        "                      'end_position': end_position,\n",
        "                      'is_impossible' : is_impossible,\n",
        "                     })\n",
        "                         \n",
        "from sklearn.model_selection import train_test_split\n",
        "import run_squad\n",
        "import modeling\n",
        "import optimization\n",
        "import tokenization\n",
        "\n",
        "train_df =  pd.read_csv('chatbot-development12.tsv', error_bad_lines=False, sep='\\t')\n",
        "print(train_df.size/3)\n",
        "train_df = train_df.sample(int(train_df.size/3))\n",
        "print(train_df.columns)\n",
        "# add doc tokens\n",
        "# add start word number of answer text\n",
        "# add end word number of answer text\n",
        "train_df_full = train_df.merge(train_df.apply(lambda row: get_squad_attributes(row), axis=1),\n",
        "                             left_index=True, right_index=True)\n",
        "\n",
        "train, test = train_test_split(train_df_full, test_size = 0.3, random_state=42)\n",
        "\n",
        "train.head()\n",
        "print(\"11111\\n\")\n",
        "test.head()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0z2T8LJmt5xS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "if tf.test.gpu_device_name():\n",
        "    print('Default GPU Device: {}'.format(tf.test.gpu_device_name()))\n",
        "else:\n",
        "    print(\"Please install GPU version of TF\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCr2uVeVw_SO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "def create_squad_example(row):\n",
        "    return run_squad.SquadExample(\n",
        "        qas_id=row['ID'],\n",
        "        question_text=row['origin_question'],\n",
        "        doc_tokens=row['doc_tokens'],\n",
        "        orig_answer_text=row['orig_answer_text'],\n",
        "        start_position=row['start_position'],\n",
        "        end_position=row['end_position'],\n",
        "        is_impossible=row['is_impossible']\n",
        "    )\n",
        "\n",
        "def create_examples(rows, set_type):\n",
        "#Generate data for the BERT model\n",
        "    guid = f'{set_type}'\n",
        "    examples = []\n",
        "    if guid == 'train':\n",
        "        for row in rows:\n",
        "            examples.append(run_squad.SquadExample(\n",
        "                    qas_id=row['ID'], # ID\n",
        "                    question_text=question_text, #TBD\n",
        "                    doc_tokens=doc_tokens, #doc_tokens\n",
        "                    orig_answer_text=orig_answer_text, # \n",
        "                    start_position=start_position,\n",
        "                    end_position=end_position,\n",
        "                    is_impossible=is_impossible)\n",
        "                           )\n",
        "    else:\n",
        "        for line in lines:\n",
        "            text_a = line\n",
        "            label = '0'\n",
        "            examples.append(\n",
        "              run_classifier.InputExample(guid=guid, text_a=text_a, text_b=None, label=label))\n",
        "    return examples\n",
        "\n",
        "# Model Hyper Parameters\n",
        "TRAIN_BATCH_SIZE = 32\n",
        "EVAL_BATCH_SIZE = 8\n",
        "LEARNING_RATE = 2e-5\n",
        "NUM_TRAIN_EPOCHS = 3.0\n",
        "WARMUP_PROPORTION = 0.1\n",
        "MAX_SEQ_LENGTH = 128\n",
        "# Model configs\n",
        "SAVE_CHECKPOINTS_STEPS = 1000 #if you wish to finetune a model on a larger dataset, use larger interval\n",
        "# each checpoint weights about 1,5gb\n",
        "ITERATIONS_PER_LOOP = 1000\n",
        "NUM_TPU_CORES = 8\n",
        "\n",
        "VOCAB_FILE = os.path.join(BERT_PRETRAINED_DIR, 'vocab.txt')\n",
        "CONFIG_FILE = os.path.join(BERT_PRETRAINED_DIR, 'bert_config.json')\n",
        "INIT_CHECKPOINT = os.path.join(BERT_PRETRAINED_DIR, 'bert_model.ckpt')\n",
        "DO_LOWER_CASE = BERT_MODEL.startswith('uncased')\n",
        "\n",
        "tokenizer = tokenization.FullTokenizer(vocab_file=VOCAB_FILE, do_lower_case=DO_LOWER_CASE)\n",
        "train_examples = train.apply(lambda row:create_squad_example(row), axis=1)\n",
        "\n",
        "tpu_cluster_resolver = tf.contrib.cluster_resolver.TPUClusterResolver(TPU_ADDRESS)\n",
        "#tpu_cluster_resolver = None #Since training will happen on GPU, we won't need a cluster resolver\n",
        "#TPUEstimator also supports training on CPU and GPU. You don't need to define a separate tf.estimator.Estimator.\n",
        "run_config = tf.contrib.tpu.RunConfig(\n",
        "    cluster=tpu_cluster_resolver,\n",
        "    model_dir=OUTPUT_DIR,\n",
        "    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS,\n",
        "    tpu_config=tf.contrib.tpu.TPUConfig(\n",
        "        iterations_per_loop=ITERATIONS_PER_LOOP,\n",
        "        num_shards=NUM_TPU_CORES,\n",
        "        per_host_input_for_training=tf.contrib.tpu.InputPipelineConfig.PER_HOST_V2))\n",
        "\n",
        "num_train_steps = int(\n",
        "    len(train_examples) / TRAIN_BATCH_SIZE * NUM_TRAIN_EPOCHS)\n",
        "num_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n",
        "\n",
        "model_fn = run_squad.model_fn_builder(\n",
        "    bert_config=modeling.BertConfig.from_json_file(CONFIG_FILE),\n",
        "    init_checkpoint=INIT_CHECKPOINT,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    num_train_steps=num_train_steps,\n",
        "    num_warmup_steps=num_warmup_steps,\n",
        "    use_tpu=True, #If False training will fall on CPU or GPU, depending on what is available  \n",
        "    use_one_hot_embeddings=True)\n",
        "\n",
        "estimator = tf.contrib.tpu.TPUEstimator(\n",
        "    use_tpu=True, #If False training will fall on CPU or GPU, depending on what is available \n",
        "    model_fn=model_fn,\n",
        "    config=run_config,\n",
        "    train_batch_size=TRAIN_BATCH_SIZE,\n",
        "    predict_batch_size=EVAL_BATCH_SIZE,\n",
        "    eval_batch_size=EVAL_BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mKkUtq5Hzcg7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "Note: You might see a message 'Running train on CPU'. \n",
        "This really just means that it's running on something other than a Cloud TPU, which includes a GPU.\n",
        "\"\"\"\n",
        "\n",
        "\n",
        "# Train the model.\n",
        "print('Please wait...')\n",
        "train_writer = run_squad.FeatureWriter(\n",
        "        filename=os.path.join(OUTPUT_DIR, \"train.tf_record\"),\n",
        "        is_training=True)\n",
        "train_features = run_squad.convert_examples_to_features(\n",
        "    train_examples, tokenizer, MAX_SEQ_LENGTH, 128, 64, True, train_writer.process_feature)\n",
        "train_writer.close()\n",
        "print('***** Started training at {} *****'.format(datetime.datetime.now()))\n",
        "print('  Num examples = {}'.format(len(train_examples)))\n",
        "print('  Batch size = {}'.format(TRAIN_BATCH_SIZE))\n",
        "tf.logging.info(\"  Num steps = %d\", num_train_steps)\n",
        "train_input_fn = run_squad.input_fn_builder(\n",
        "    input_file=train_writer.filename,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=True,\n",
        "    drop_remainder=True)\n",
        "estimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\n",
        "print('***** Finished training at {} *****'.format(datetime.datetime.now()))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EhaSM5k55uoq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def create_test_squad_example(row):\n",
        "    return run_squad.SquadExample(\n",
        "        qas_id=row['ID'],\n",
        "        question_text=row['origin_question'],\n",
        "        doc_tokens=row['doc_tokens'],\n",
        "        orig_answer_text=\"\",\n",
        "        start_position=-1,\n",
        "        end_position=-1,\n",
        "        is_impossible=row['is_impossible']\n",
        "    )\n",
        "\n",
        "eval_examples = test.apply(lambda row:create_test_squad_example(row), axis=1)\n",
        "\n",
        "eval_writer = run_squad.FeatureWriter(\n",
        "    filename=os.path.join(OUTPUT_DIR, \"eval.tf_record\"),\n",
        "    is_training=False)\n",
        "\n",
        "eval_features = []\n",
        "def append_feature(feature):\n",
        "    eval_features.append(feature)\n",
        "    eval_writer.process_feature(feature)\n",
        "    \n",
        "run_squad.convert_examples_to_features(\n",
        "        examples=eval_examples,\n",
        "        tokenizer=tokenizer,\n",
        "        max_seq_length=MAX_SEQ_LENGTH,\n",
        "        doc_stride=128,\n",
        "        max_query_length=64,\n",
        "        is_training=False,\n",
        "        output_fn=append_feature)\n",
        "\n",
        "eval_writer.close()\n",
        "\n",
        "tf.logging.info(\"***** Running predictions *****\")\n",
        "tf.logging.info(\"  Num orig examples = %d\", len(eval_examples))\n",
        "tf.logging.info(\"  Num features = %d\", len(eval_features))\n",
        "predict_input_fn = run_squad.input_fn_builder(\n",
        "    input_file=eval_writer.filename,\n",
        "    seq_length=MAX_SEQ_LENGTH,\n",
        "    is_training=False,\n",
        "    drop_remainder=False)\n",
        "\n",
        "all_results = []\n",
        "for result in estimator.predict(predict_input_fn, yield_single_examples=True):\n",
        "    if len(all_results) % 1000 == 0:\n",
        "        tf.logging.info(\"Processing example: %d\" % (len(all_results)))\n",
        "    unique_id = int(result[\"unique_ids\"])\n",
        "    start_logits = [float(x) for x in result[\"start_logits\"].flat]\n",
        "    end_logits = [float(x) for x in result[\"end_logits\"].flat]\n",
        "    all_results.append(\n",
        "          run_squad.RawResult(\n",
        "              unique_id=unique_id,\n",
        "              start_logits=start_logits,\n",
        "              end_logits=end_logits))\n",
        "\n",
        "output_prediction_file = os.path.join(OUTPUT_DIR, \"predictions.json\")\n",
        "output_nbest_file = os.path.join(OUTPUT_DIR, \"nbest_predictions.json\")\n",
        "output_null_log_odds_file = os.path.join(OUTPUT_DIR, \"null_odds.json\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bk3oy_mJgyRu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def write_predictions(all_examples, all_features, all_results, n_best_size,\n",
        "                      max_answer_length, do_lower_case, output_prediction_file,\n",
        "                      output_nbest_file, output_null_log_odds_file):\n",
        "  \"\"\"Write final predictions to the json file and log-odds of null if needed.\"\"\"\n",
        "  tf.logging.info(\"Writing predictions to: %s\" % (output_prediction_file))\n",
        "  tf.logging.info(\"Writing nbest to: %s\" % (output_nbest_file))\n",
        "\n",
        "  example_index_to_features = collections.defaultdict(list)\n",
        "  for feature in all_features:\n",
        "    example_index_to_features[feature.example_index].append(feature)\n",
        "\n",
        "  unique_id_to_result = {}\n",
        "  for result in all_results:\n",
        "    unique_id_to_result[result.unique_id] = result\n",
        "    _PrelimPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "      \"PrelimPrediction\",\n",
        "      [\"feature_index\", \"start_index\", \"end_index\", \"start_logit\", \"end_logit\"])\n",
        "\n",
        "  all_predictions = collections.OrderedDict()\n",
        "  all_nbest_json = collections.OrderedDict()\n",
        "  scores_diff_json = collections.OrderedDict()\n",
        "\n",
        "  for (example_index, example) in enumerate(all_examples):\n",
        "    features = example_index_to_features[example_index]\n",
        "\n",
        "    prelim_predictions = []\n",
        "    # keep track of the minimum score of null start+end of position 0\n",
        "    score_null = 1000000  # large and positive\n",
        "    min_null_feature_index = 0  # the paragraph slice with min mull score\n",
        "    null_start_logit = 0  # the start logit at the slice with min null score\n",
        "    null_end_logit = 0  # the end logit at the slice with min null score\n",
        "    for (feature_index, feature) in enumerate(features):\n",
        "      result = unique_id_to_result[feature.unique_id]\n",
        "      start_indexes = _get_best_indexes(result.start_logits, n_best_size)\n",
        "      end_indexes = _get_best_indexes(result.end_logits, n_best_size)\n",
        "      # if we could have irrelevant answers, get the min score of irrelevant\n",
        "      for start_index in start_indexes:\n",
        "        for end_index in end_indexes:\n",
        "              # We could hypothetically create invalid predictions, e.g., predict\n",
        "          # that the start of the span is in the question. We throw out all\n",
        "          # invalid predictions.\n",
        "          if start_index >= len(feature.tokens):\n",
        "            continue\n",
        "          if end_index >= len(feature.tokens):\n",
        "            continue\n",
        "          if start_index not in feature.token_to_orig_map:\n",
        "            continue\n",
        "          if end_index not in feature.token_to_orig_map:\n",
        "            continue\n",
        "          if not feature.token_is_max_context.get(start_index, False):\n",
        "            continue\n",
        "          if end_index < start_index:\n",
        "            continue\n",
        "          length = end_index - start_index + 1\n",
        "          if length > max_answer_length:\n",
        "            continue\n",
        "          prelim_predictions.append(\n",
        "              _PrelimPrediction(\n",
        "                  feature_index=feature_index,\n",
        "                  start_index=start_index,\n",
        "                  end_index=end_index,\n",
        "                  start_logit=result.start_logits[start_index],\n",
        "                  end_logit=result.end_logits[end_index]))\n",
        "    prelim_predictions = sorted(\n",
        "        prelim_predictions,\n",
        "        key=lambda x: (x.start_logit + x.end_logit),\n",
        "        reverse=True)\n",
        "\n",
        "    _NbestPrediction = collections.namedtuple(  # pylint: disable=invalid-name\n",
        "        \"NbestPrediction\", [\"text\", \"start_logit\", \"end_logit\"])\n",
        "\n",
        "    seen_predictions = {}\n",
        "    nbest = []\n",
        "    for pred in prelim_predictions:\n",
        "      if len(nbest) >= n_best_size:\n",
        "        break\n",
        "      feature = features[pred.feature_index]\n",
        "      if pred.start_index > 0:  # this is a non-null prediction\n",
        "        tok_tokens = feature.tokens[pred.start_index:(pred.end_index + 1)]\n",
        "        orig_doc_start = feature.token_to_orig_map[pred.start_index]\n",
        "        orig_doc_end = feature.token_to_orig_map[pred.end_index]\n",
        "        orig_tokens = example.doc_tokens[orig_doc_start:(orig_doc_end + 1)]\n",
        "        tok_text = \" \".join(tok_tokens)\n",
        "\n",
        "        # De-tokenize WordPieces that have been split off.\n",
        "        tok_text = tok_text.replace(\" ##\", \"\")\n",
        "        tok_text = tok_text.replace(\"##\", \"\")\n",
        "\n",
        "        # Clean whitespace\n",
        "        tok_text = tok_text.strip()\n",
        "        tok_text = \" \".join(tok_text.split())\n",
        "        orig_text = \" \".join(orig_tokens)\n",
        "\n",
        "        final_text = run_squad.get_final_text(tok_text, orig_text, do_lower_case)\n",
        "        if final_text in seen_predictions:\n",
        "          continue\n",
        "\n",
        "        seen_predictions[final_text] = True\n",
        "      else:\n",
        "        final_text = \"\"\n",
        "        seen_predictions[final_text] = True\n",
        "\n",
        "      nbest.append(\n",
        "          _NbestPrediction(\n",
        "              text=final_text,\n",
        "              start_logit=pred.start_logit,\n",
        "              end_logit=pred.end_logit))\n",
        "\n",
        "    # In very rare edge cases we could have no valid predictions. So we\n",
        "    # just create a nonce prediction in this case to avoid failure.\n",
        "    if not nbest:\n",
        "      nbest.append(\n",
        "          _NbestPrediction(text=\"empty\", start_logit=0.0, end_logit=0.0))\n",
        "\n",
        "    assert len(nbest) >= 1\n",
        "\n",
        "    total_scores = []\n",
        "    best_non_null_entry = None\n",
        "    for entry in nbest:\n",
        "      total_scores.append(entry.start_logit + entry.end_logit)\n",
        "      if not best_non_null_entry:\n",
        "        if entry.text:\n",
        "          best_non_null_entry = entry\n",
        "\n",
        "    probs = run_squad._compute_softmax(total_scores)\n",
        "\n",
        "    nbest_json = []\n",
        "    for (i, entry) in enumerate(nbest):\n",
        "      output = collections.OrderedDict()\n",
        "      output[\"text\"] = entry.text\n",
        "      output[\"probability\"] = probs[i]\n",
        "      output[\"start_logit\"] = entry.start_logit\n",
        "      output[\"end_logit\"] = entry.end_logit\n",
        "      nbest_json.append(output)\n",
        "\n",
        "    assert len(nbest_json) >= 1\n",
        "\n",
        "    all_predictions[example.qas_id] = nbest_json[0][\"text\"]\n",
        "\n",
        "    all_nbest_json[example.qas_id] = nbest_json\n",
        "\n",
        "  with tf.gfile.GFile(output_prediction_file, \"w\") as writer:\n",
        "    writer.write(json.dumps(all_predictions, indent=4) + \"\\n\")\n",
        "\n",
        "  with tf.gfile.GFile(output_nbest_file, \"w\") as writer:\n",
        "    writer.write(json.dumps(all_nbest_json, indent=4) + \"\\n\")\n",
        "    \n",
        "\n",
        "def _get_best_indexes(logits, n_best_size):\n",
        "  \"\"\"Get the n-best logits from a list.\"\"\"\n",
        "  index_and_score = sorted(enumerate(logits), key=lambda x: x[1], reverse=True)\n",
        "\n",
        "  best_indexes = []\n",
        "  for i in range(len(index_and_score)):\n",
        "    if i >= n_best_size:\n",
        "      break\n",
        "    best_indexes.append(index_and_score[i][0])\n",
        "  return best_indexes\n",
        "\n",
        "import collections\n",
        "import json\n",
        "import tensorflow as tf\n",
        "\n",
        "tf.app.flags.DEFINE_string('f', '', 'kernel')\n",
        "\n",
        "#flags.DEFINE_bool(\n",
        "#    \"verbose_logging\", True,\n",
        "#    \"Whether to log verbosely. Should be True if logginf versbosely\")\n",
        "\n",
        "write_predictions(eval_examples, eval_features, all_results,\n",
        "                      20, 30,\n",
        "                      True, output_prediction_file,\n",
        "                      output_nbest_file, output_null_log_odds_file)\n",
        "\n",
        "train_df_full[train_df_full.ID == \"1446\"]\n"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}